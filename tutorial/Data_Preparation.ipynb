{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "751f2ada",
   "metadata": {},
   "source": [
    "# Data Preparation Tutorial\n",
    "\n",
    "**Purpose:** A sample case of doing data preprocessing using example of the dataset presented in the paper \"Wearable Physiological Signals under Acute Stress and Exercise Conditions\"\n",
    "\n",
    "**What this notebook contains**\n",
    "- Clear modular functions for loading, preprocessing, feature extraction, combination, and saving.\n",
    "- Relative paths only — saves outputs under `./Individual Dataset/`.\n",
    "\n",
    "Run the notebook cells sequentially. The main entrypoint is `run_preprocessing_pipeline()`\n",
    "which will create the following files inside `./Individual Dataset/`:\n",
    "\n",
    "```\n",
    "Raw_EDA.csv\n",
    "Raw_PPG.csv\n",
    "Features_EDA.csv\n",
    "Features_PPG.csv\n",
    "Features_Combined.csv\n",
    "Features_FourClass_PPG.csv\n",
    "Features_FourClass_EDA.csv\n",
    "Features_FourClass_Combined.csv\n",
    "Features_Combined_Demographics.csv (if applicable)\n",
    "Features_PPG_Demographics.csv (if applicable)\n",
    "Features_EDA_Demographics.csv (if applicable)\n",
    "```\n",
    "\n",
    "## Overview\n",
    "\n",
    "The preprocessing pipeline includes:\n",
    "- Loading and organizing raw physiological data\n",
    "- Segmenting data based on experimental protocols\n",
    "- Extracting meaningful features from EDA and PPG signals\n",
    "- Categorizing data by arousal and valence dimensions\n",
    "- Creating four-class emotional states classification\n",
    "- Saving processed datasets for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import neurokit2 as nk\n",
    "import scipy.stats as stats\n",
    "import cvxEDA.src.cvxEDA as cvxEDA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "label-processing",
   "metadata": {},
   "source": [
    "## 2. Label Processing Functions\n",
    "\n",
    "These functions process the stress level labels and create event sequences for different experimental conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e309f055",
   "metadata": {},
   "source": [
    "## Dataset Overview\n",
    "\n",
    "The **Exercise dataset** (Hongn et al., 2025) contains physiological data from three experimental conditions:\n",
    "\n",
    "1. **Stress Induction Protocol** (36 participants)  \n",
    "2. **Aerobic Exercise** (30 participants)  \n",
    "3. **Anaerobic Exercise** (31 participants)  \n",
    "\n",
    "Data was collected using **Empatica E4** wearable devices capturing **EDA**, **PPG**, and other physiological signals.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aee61fd",
   "metadata": {},
   "source": [
    "## Two-Class Labeling Strategy\n",
    "\n",
    "### Arousal Categories (High / Low)\n",
    "\n",
    "**Stress Protocol:**\n",
    "- **High Arousal (1)**: Tasks designed to elicit stress responses  \n",
    "  - Stroop Test  \n",
    "  - Trier Mental Challenge Test (mathematical tasks with annoying audio)  \n",
    "  - Controversial opinion vocalization  \n",
    "  - Backward counting from 1022 in decrements of 13  \n",
    "- **Low Arousal (0)**: Baseline and rest periods  \n",
    "\n",
    "**Exercise Sessions:**\n",
    "- **High Arousal (1)**: Active cycling periods  \n",
    "- **Low Arousal (0)**: Baseline, warm-up, cool-down, and rest periods  \n",
    "\n",
    "---\n",
    "\n",
    "### Valence Categories (Positive / Negative)\n",
    "\n",
    "**Stress Protocol:**\n",
    "- **Positive Valence (1)**: Low stress scores from self-reports  \n",
    "- **Negative Valence (0)**: High stress scores from self-reports  \n",
    "\n",
    "**Aerobic Exercise:**\n",
    "- **Positive Valence (1)**: Cycling up to 85 rpm speed  \n",
    "- **Negative Valence (0)**: Cycling above 85 rpm speed  \n",
    "\n",
    "**Anaerobic Exercise:**\n",
    "- **Positive Valence (1)**: Initial two sprints  \n",
    "- **Negative Valence (0)**: Later sprints  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399af01c",
   "metadata": {},
   "source": [
    "## Four-Class Emotional State Mapping\n",
    "\n",
    "The four-class system combines arousal and valence dimensions to create comprehensive emotional states:\n",
    "\n",
    "### Class 0: Low Arousal, Low Valence (0, 0)\n",
    "**Physiological Interpretation:** Calm-negative state  \n",
    "- **Stress Protocol:** Low stress but negative affective state  \n",
    "- **Aerobic Exercise:** Rest periods with negative valence (post-high intensity)  \n",
    "- **Anaerobic Exercise:** Rest periods between later sprints  \n",
    "\n",
    "---\n",
    "\n",
    "### Class 1: Low Arousal, High Valence (0, 1)\n",
    "**Physiological Interpretation:** Calm-positive state  \n",
    "- **Stress Protocol:** Low stress with positive affective state  \n",
    "- **Aerobic Exercise:** Warm-up and initial cycling phases  \n",
    "- **Anaerobic Exercise:** Rest periods between initial sprints  \n",
    "\n",
    "---\n",
    "\n",
    "### Class 2: High Arousal, Low Valence (1, 0)\n",
    "**Physiological Interpretation:** Agitated-negative state  \n",
    "- **Stress Protocol:** High stress conditions (Stroop, TMCT, controversial topics)  \n",
    "- **Aerobic Exercise:** High-intensity cycling (>85 rpm)  \n",
    "- **Anaerobic Exercise:** Later sprint sessions  \n",
    "\n",
    "---\n",
    "\n",
    "### Class 3: High Arousal, High Valence (1, 1)\n",
    "**Physiological Interpretation:** Excited-positive state  \n",
    "- **Stress Protocol:** Not typically present in stress induction  \n",
    "- **Aerobic Exercise:** Moderate-intensity cycling (≤85 rpm)  \n",
    "- **Anaerobic Exercise:** Initial sprint sessions  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "label-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stress_labels():\n",
    "    \"\"\"Load and process stress level labels from CSV files.\"\"\"\n",
    "    level1_csv = pd.read_csv(LABEL_FILES[\"stress_v1\"])\n",
    "    level2_csv = pd.read_csv(LABEL_FILES[\"stress_v2\"])\n",
    "    return level1_csv, level2_csv\n",
    "\n",
    "def create_stress_bin(level1_csv, level2_csv, data_folder):\n",
    "    \"\"\"Create stress event sequences for each participant.\"\"\"\n",
    "    pids = [f.name for f in os.scandir(data_folder) if f.is_dir()]\n",
    "    stress_bin = {}\n",
    "    \n",
    "    for pid in pids:\n",
    "        if pid == \"f14_b\":\n",
    "            continue\n",
    "        elif pid == \"f14_a\":\n",
    "            # Handle special case for f14\n",
    "            pid_df = level2_csv[level2_csv['Unnamed: 0'] == \"f14\"]\n",
    "            arousal_seq, valence_seq = create_stress_sequence_v2(pid_df)\n",
    "            stress_bin[\"f14\"] = (arousal_seq, valence_seq)\n",
    "        elif pid[0] == \"f\":\n",
    "            # Version 2 participants\n",
    "            pid_df = level2_csv[level2_csv['Unnamed: 0'] == pid]\n",
    "            arousal_seq, valence_seq = create_stress_sequence_v2(pid_df)\n",
    "            stress_bin[pid] = (arousal_seq, valence_seq)\n",
    "        elif pid[0] == \"S\":\n",
    "            # Version 1 participants\n",
    "            pid_df = level1_csv[level1_csv['Unnamed: 0'] == pid]\n",
    "            arousal_seq, valence_seq = create_stress_sequence_v1(pid_df)\n",
    "            stress_bin[pid] = (arousal_seq, valence_seq)\n",
    "        else:\n",
    "            print(f\"Unknown PID format: {pid}\")\n",
    "    \n",
    "    return stress_bin\n",
    "\n",
    "def create_stress_sequence_v1(pid_df):\n",
    "    \"\"\"Create stress event sequence for version 1 participants.\"\"\"\n",
    "    arousal_seq = [\n",
    "        (3, pid_df['Baseline'].tolist()[0]), \n",
    "        (5, pid_df['Stroop'].tolist()[0]), \n",
    "        (10, pid_df['First Rest'].tolist()[0]), \n",
    "        (13, pid_df['TMCT'].tolist()[0]), \n",
    "        (18, pid_df['Second Rest'].tolist()[0]), \n",
    "        (18.5, pid_df['Real Opinion'].tolist()[0]), \n",
    "        (19, pid_df['Opposite Opinion'].tolist()[0]), \n",
    "        (19.5, pid_df['Subtract'].tolist()[0])\n",
    "    ]\n",
    "    arousal_seq = [(i[0], 1 if i[1] > 4 else 0) for i in arousal_seq]\n",
    "    \n",
    "    valence_seq = [\n",
    "        (3, pid_df['Baseline'].tolist()[0]), \n",
    "        (5, pid_df['Stroop'].tolist()[0]), \n",
    "        (10, pid_df['First Rest'].tolist()[0]), \n",
    "        (13, pid_df['TMCT'].tolist()[0]), \n",
    "        (18, pid_df['Second Rest'].tolist()[0]), \n",
    "        (18.5, pid_df['Real Opinion'].tolist()[0]), \n",
    "        (19, pid_df['Opposite Opinion'].tolist()[0]), \n",
    "        (19.5, pid_df['Subtract'].tolist()[0])\n",
    "    ]\n",
    "    valence_seq = [(i[0], 0 if i[1] > 3 else 1) for i in valence_seq]\n",
    "    \n",
    "    return arousal_seq, valence_seq\n",
    "\n",
    "def create_stress_sequence_v2(pid_df):\n",
    "    \"\"\"Create stress event sequence for version 2 participants.\"\"\"\n",
    "    arousal_seq = [\n",
    "        (3, pid_df['Baseline'].tolist()[0]), \n",
    "        (6, pid_df['TMCT'].tolist()[0]), \n",
    "        (16, pid_df['First Rest'].tolist()[0]), \n",
    "        (16.5, pid_df['Real Opinion'].tolist()[0]), \n",
    "        (17, pid_df['Opposite Opinion'].tolist()[0]), \n",
    "        (27, pid_df['Second Rest'].tolist()[0]), \n",
    "        (27.5, pid_df['Subtract'].tolist()[0])\n",
    "    ]\n",
    "    arousal_seq = [(i[0], 1 if i[1] > 4 else 0) for i in arousal_seq]\n",
    "    \n",
    "    valence_seq = [\n",
    "        (3, pid_df['Baseline'].tolist()[0]), \n",
    "        (6, pid_df['TMCT'].tolist()[0]), \n",
    "        (16, pid_df['First Rest'].tolist()[0]), \n",
    "        (16.5, pid_df['Real Opinion'].tolist()[0]), \n",
    "        (17, pid_df['Opposite Opinion'].tolist()[0]), \n",
    "        (27, pid_df['Second Rest'].tolist()[0]), \n",
    "        (27.5, pid_df['Subtract'].tolist()[0])\n",
    "    ]\n",
    "    valence_seq = [(i[0], 0 if i[1] > 3 else 1) for i in valence_seq]\n",
    "    \n",
    "    return arousal_seq, valence_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading",
   "metadata": {},
   "source": [
    "## 4. Data Loading and Segmentation\n",
    "\n",
    "These functions handle loading raw physiological data and segmenting it according to experimental protocols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_physiological_data(pid, condition):\n",
    "    \"\"\"Load EDA and PPG data for a given participant and condition.\"\"\"\n",
    "    data_folder = DATA_DIRS[condition]\n",
    "    \n",
    "    try:\n",
    "        eda_data = pd.read_csv(data_folder / pid / \"EDA.csv\")\n",
    "        ppg_data = pd.read_csv(data_folder / pid / \"BVP.csv\")\n",
    "        \n",
    "        # Extract data values (assuming first column contains the data)\n",
    "        eda_values = eda_data[eda_data.columns[0]].tolist()[1:]  # Skip header\n",
    "        ppg_values = ppg_data[ppg_data.columns[0]].tolist()[1:]  # Skip header\n",
    "        \n",
    "        return eda_values, ppg_values\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Data not found for {pid} in {condition}\")\n",
    "        return None, None\n",
    "\n",
    "def segment_data(data, event_sequence, sampling_rate, start_offset=0):\n",
    "    \"\"\"Segment data based on event sequence timings.\"\"\"\n",
    "    segments = []\n",
    "    \n",
    "    for i in range(len(event_sequence)):\n",
    "        current_time = event_sequence[i][0]\n",
    "        \n",
    "        if i == 0:\n",
    "            start_idx = start_offset\n",
    "        else:\n",
    "            prev_time = event_sequence[i-1][0]\n",
    "            start_idx = int(prev_time * 60 * sampling_rate)\n",
    "        \n",
    "        if i == len(event_sequence) - 1:\n",
    "            end_idx = len(data)\n",
    "        else:\n",
    "            end_idx = int(current_time * 60 * sampling_rate)\n",
    "        \n",
    "        segment = data[start_idx:end_idx]\n",
    "        segments.append(segment)\n",
    "    \n",
    "    return segments\n",
    "\n",
    "def create_segmented_dataset(condition_bin, condition, eda_sampling_rate=4, ppg_sampling_rate=64):\n",
    "    \"\"\"Create segmented dataset for a given condition.\"\"\"\n",
    "    raw_eda = pd.DataFrame(columns=[\"PID\", \"arousal_category\", \"valence_category\", \"Data\"])\n",
    "    raw_ppg = pd.DataFrame(columns=[\"PID\", \"arousal_category\", \"valence_category\", \"Data\"])\n",
    "    \n",
    "    for pid in condition_bin.keys():\n",
    "        eda_data, ppg_data = load_physiological_data(pid, condition)\n",
    "        \n",
    "        if eda_data is None or ppg_data is None:\n",
    "            continue\n",
    "        \n",
    "        arousal_seq = condition_bin[pid][0]\n",
    "        valence_seq = condition_bin[pid][1]\n",
    "        \n",
    "        # Apply condition-specific preprocessing\n",
    "        eda_processed, ppg_processed = preprocess_condition_data(pid, condition, eda_data, ppg_data)\n",
    "        \n",
    "        # Segment data\n",
    "        eda_segments = segment_data(eda_processed, arousal_seq, eda_sampling_rate)\n",
    "        ppg_segments = segment_data(ppg_processed, arousal_seq, ppg_sampling_rate)\n",
    "        \n",
    "        # Add segments to dataset\n",
    "        for i, (eda_segment, ppg_segment) in enumerate(zip(eda_segments, ppg_segments)):\n",
    "            arousal_cat = arousal_seq[i][1]\n",
    "            valence_cat = valence_seq[i][1]\n",
    "            \n",
    "            eda_row = {\"PID\": pid, \"arousal_category\": arousal_cat, \"valence_category\": valence_cat, \"Data\": eda_segment}\n",
    "            ppg_row = {\"PID\": pid, \"arousal_category\": arousal_cat, \"valence_category\": valence_cat, \"Data\": ppg_segment}\n",
    "            \n",
    "            raw_eda = pd.concat([raw_eda, pd.DataFrame([eda_row])], ignore_index=True)\n",
    "            raw_ppg = pd.concat([raw_ppg, pd.DataFrame([ppg_row])], ignore_index=True)\n",
    "    \n",
    "    return raw_eda, raw_ppg\n",
    "\n",
    "def preprocess_condition_data(pid, condition, eda_data, ppg_data):\n",
    "    \"\"\"Apply condition-specific preprocessing to data.\"\"\"\n",
    "    # This function should implement the condition-specific trimming and preprocessing\n",
    "    # shown in the original code for different PIDs and conditions\n",
    "    \n",
    "    # Placeholder implementation - extend this based on your specific preprocessing needs\n",
    "    if condition == \"stress\":\n",
    "        # Apply stress-specific preprocessing\n",
    "        if pid in [\"f01\", \"f02\", \"f03\", \"f04\"]:\n",
    "            # Example: Trim specific ranges for certain participants\n",
    "            eda_processed = eda_data[500:6500]\n",
    "            ppg_processed = ppg_data[int(500*64/4):int(6500*64/4)]\n",
    "        else:\n",
    "            eda_processed = eda_data\n",
    "            ppg_processed = ppg_data\n",
    "    else:\n",
    "        eda_processed = eda_data\n",
    "        ppg_processed = ppg_data\n",
    "    \n",
    "    return eda_processed, ppg_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-extraction",
   "metadata": {},
   "source": [
    "## 5. Feature Extraction\n",
    "\n",
    "These functions extract meaningful features from EDA and PPG signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_eda_features(eda_signal, sampling_rate=4):\n",
    "    \"\"\"Extract features from EDA signal.\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        # Basic statistical features\n",
    "        features['eda_mean'] = np.mean(eda_signal)\n",
    "        features['eda_std'] = np.std(eda_signal)\n",
    "        features['eda_skew'] = stats.skew(eda_signal)\n",
    "        features['eda_kurtosis'] = stats.kurtosis(eda_signal)\n",
    "        \n",
    "        # NeuroKit2 EDA analysis\n",
    "        eda_cleaned = nk.eda_clean(eda_signal, sampling_rate=sampling_rate)\n",
    "        eda_decomposed = nk.eda_phasic(eda_cleaned, sampling_rate=sampling_rate)\n",
    "        \n",
    "        # Phasic and tonic components\n",
    "        features['eda_tonic_mean'] = np.mean(eda_decomposed['EDA_Tonic'])\n",
    "        features['eda_phasic_mean'] = np.mean(eda_decomposed['EDA_Phasic'])\n",
    "        \n",
    "        # Peak detection\n",
    "        signals, info = nk.eda_peaks(eda_decomposed['EDA_Phasic'], sampling_rate=sampling_rate)\n",
    "        features['eda_peaks_count'] = len(info['SCR_Peaks'])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in EDA feature extraction: {e}\")\n",
    "        # Set default values for failed extractions\n",
    "        for key in ['eda_mean', 'eda_std', 'eda_skew', 'eda_kurtosis', \n",
    "                   'eda_tonic_mean', 'eda_phasic_mean', 'eda_peaks_count']:\n",
    "            features[key] = 0.0\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_ppg_features(ppg_signal, sampling_rate=64):\n",
    "    \"\"\"Extract features from PPG signal.\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        # Basic statistical features\n",
    "        features['ppg_mean'] = np.mean(ppg_signal)\n",
    "        features['ppg_std'] = np.std(ppg_signal)\n",
    "        features['ppg_skew'] = stats.skew(ppg_signal)\n",
    "        features['ppg_kurtosis'] = stats.kurtosis(ppg_signal)\n",
    "        \n",
    "        # Heart rate variability features\n",
    "        ppg_cleaned = nk.ppg_clean(ppg_signal, sampling_rate=sampling_rate)\n",
    "        signals, info = nk.ppg_process(ppg_cleaned, sampling_rate=sampling_rate)\n",
    "        \n",
    "        # Heart rate features\n",
    "        features['hr_mean'] = np.mean(signals['PPG_Rate'])\n",
    "        features['hr_std'] = np.std(signals['PPG_Rate'])\n",
    "        \n",
    "        # Additional PPG features\n",
    "        hrv_features = nk.ppg_analyze(signals, sampling_rate=sampling_rate)\n",
    "        if not hrv_features.empty:\n",
    "            for col in hrv_features.columns:\n",
    "                features[f'ppg_{col}'] = hrv_features[col].iloc[0]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in PPG feature extraction: {e}\")\n",
    "        # Set default values for failed extractions\n",
    "        for key in ['ppg_mean', 'ppg_std', 'ppg_skew', 'ppg_kurtosis', 'hr_mean', 'hr_std']:\n",
    "            features[key] = 0.0\n",
    "    \n",
    "    return features\n",
    "\n",
    "def create_feature_dataset(raw_data, feature_extraction_func, signal_type):\n",
    "    \"\"\"Create feature dataset from raw segmented data.\"\"\"\n",
    "    features_list = []\n",
    "    \n",
    "    for idx, row in raw_data.iterrows():\n",
    "        pid = row['PID']\n",
    "        arousal = row['arousal_category']\n",
    "        valence = row['valence_category']\n",
    "        signal_data = row['Data']\n",
    "        \n",
    "        # Extract features\n",
    "        features = feature_extraction_func(signal_data)\n",
    "        \n",
    "        # Add metadata\n",
    "        features['PID'] = pid\n",
    "        features['arousal_category'] = arousal\n",
    "        features['valence_category'] = valence\n",
    "        features['four_class_category'] = create_four_class_category(arousal, valence)\n",
    "        \n",
    "        features_list.append(features)\n",
    "    \n",
    "    feature_df = pd.DataFrame(features_list)\n",
    "    return feature_df\n",
    "\n",
    "def create_four_class_category(arousal, valence):\n",
    "    \"\"\"Create four-class emotional state category.\"\"\"\n",
    "    # [low arousal low valence, low arousal high valence, high arousal low valence, high arousal high valence]\n",
    "    if arousal == 0 and valence == 0:\n",
    "        return 0  # Low arousal, low valence\n",
    "    elif arousal == 0 and valence == 1:\n",
    "        return 1  # Low arousal, high valence\n",
    "    elif arousal == 1 and valence == 0:\n",
    "        return 2  # High arousal, low valence\n",
    "    else:  # arousal == 1 and valence == 1\n",
    "        return 3  # High arousal, high valence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main-pipeline",
   "metadata": {},
   "source": [
    "## 6. Main Processing Pipeline\n",
    "\n",
    "This section runs the complete preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_complete_pipeline():\n",
    "    \"\"\"Run the complete data preprocessing pipeline.\"\"\"\n",
    "    \n",
    "    print(\"Starting data preprocessing pipeline...\")\n",
    "    \n",
    "    # Step 1: Load and process labels\n",
    "    print(\"Step 1: Loading stress labels...\")\n",
    "    level1_csv, level2_csv = load_stress_labels()\n",
    "    stress_bin = create_stress_bin(level1_csv, level2_csv, DATA_DIRS[\"stress\"])\n",
    "    \n",
    "    # Define aerobic and anaerobic sequences (simplified - extend as needed)\n",
    "    aerobic_bin = create_aerobic_sequences(stress_bin)\n",
    "    anaerobic_bin = create_anaerobic_sequences(stress_bin)\n",
    "    \n",
    "    # Step 2: Create segmented datasets\n",
    "    print(\"Step 2: Creating segmented datasets...\")\n",
    "    \n",
    "    # Process stress condition\n",
    "    raw_eda_stress, raw_ppg_stress = create_segmented_dataset(stress_bin, \"stress\")\n",
    "    \n",
    "    # Process aerobic condition\n",
    "    raw_eda_aerobic, raw_ppg_aerobic = create_segmented_dataset(aerobic_bin, \"aerobic\")\n",
    "    \n",
    "    # Process anaerobic condition\n",
    "    raw_eda_anaerobic, raw_ppg_anaerobic = create_segmented_dataset(anaerobic_bin, \"anaerobic\")\n",
    "    \n",
    "    # Combine all conditions\n",
    "    raw_eda_combined = pd.concat([raw_eda_stress, raw_eda_aerobic, raw_eda_anaerobic], ignore_index=True)\n",
    "    raw_ppg_combined = pd.concat([raw_ppg_stress, raw_ppg_aerobic, raw_ppg_anaerobic], ignore_index=True)\n",
    "    \n",
    "    # Step 3: Extract features\n",
    "    print(\"Step 3: Extracting features...\")\n",
    "    \n",
    "    features_eda = create_feature_dataset(raw_eda_combined, extract_eda_features, \"EDA\")\n",
    "    features_ppg = create_feature_dataset(raw_ppg_combined, extract_ppg_features, \"PPG\")\n",
    "    \n",
    "    # Step 4: Create combined features\n",
    "    print(\"Step 4: Creating combined features...\")\n",
    "    features_combined = combine_features(features_eda, features_ppg)\n",
    "    \n",
    "    # Step 5: Create four-class datasets\n",
    "    print(\"Step 5: Creating four-class datasets...\")\n",
    "    features_fourclass_eda = create_four_class_dataset(features_eda)\n",
    "    features_fourclass_ppg = create_four_class_dataset(features_ppg)\n",
    "    features_fourclass_combined = create_four_class_dataset(features_combined)\n",
    "    \n",
    "    # Step 6: Save all datasets\n",
    "    print(\"Step 6: Saving datasets...\")\n",
    "    save_datasets(\n",
    "        raw_eda_combined, raw_ppg_combined,\n",
    "        features_eda, features_ppg, features_combined,\n",
    "        features_fourclass_eda, features_fourclass_ppg, features_fourclass_combined\n",
    "    )\n",
    "    \n",
    "    print(\"Pipeline completed successfully!\")\n",
    "    \n",
    "    return {\n",
    "        'raw_eda': raw_eda_combined,\n",
    "        'raw_ppg': raw_ppg_combined,\n",
    "        'features_eda': features_eda,\n",
    "        'features_ppg': features_ppg,\n",
    "        'features_combined': features_combined,\n",
    "        'features_fourclass_eda': features_fourclass_eda,\n",
    "        'features_fourclass_ppg': features_fourclass_ppg,\n",
    "        'features_fourclass_combined': features_fourclass_combined\n",
    "    }\n",
    "\n",
    "def create_aerobic_sequences(stress_bin):\n",
    "    \"\"\"Create aerobic event sequences.\"\"\"\n",
    "    aerobic_bin = {}\n",
    "    for pid in stress_bin.keys():\n",
    "        if pid[0] == \"f\":\n",
    "            arousal_seq = [(4.5, 0), (6.75, 0), (8.25, 1), (9.75, 1), (11.25, 1), (22.5, 1), (27, 1), (30, 0), (32, 0)]\n",
    "            valence_seq = [(4.5, 1), (6.75, 1), (8.25, 1), (9.75, 1), (11.25, 1), (22.5, 0), (27, 0), (30, 0), (32, 1)]\n",
    "        else:  # S participants\n",
    "            arousal_seq = [(3, 0), (6, 0), (9, 1), (12, 1), (15, 1), (18, 1), (21, 1), (23, 1), (25, 1), (27, 1), (29, 1), (33, 0), (35, 0)]\n",
    "            valence_seq = [(3, 1), (6, 1), (9, 1), (12, 1), (15, 1), (18, 0), (21, 0), (23, 0), (25, 0), (27, 0), (29, 0), (33, 0), (35, 1)]\n",
    "        aerobic_bin[pid] = (arousal_seq, valence_seq)\n",
    "    return aerobic_bin\n",
    "\n",
    "def create_anaerobic_sequences(stress_bin):\n",
    "    \"\"\"Create anaerobic event sequences.\"\"\"\n",
    "    anaerobic_bin = {}\n",
    "    for pid in stress_bin.keys():\n",
    "        if pid[0] == \"f\":\n",
    "            arousal_seq = [(4.5, 0), (9, 0), (9.75, 1), (14, 0), (14.75, 1), (18.5, 0), (19.25, 1), (23, 0), (23.75, 1), (27.5, 0), (29.5, 0)]\n",
    "            valence_seq = [(4.5, 1), (9, 1), (9.75, 1), (14, 1), (14.75, 1), (18.5, 1), (19.25, 0), (23, 0), (23.75, 0), (27.5, 0), (29.5, 1)]\n",
    "        else:  # S participants\n",
    "            arousal_seq = [(3, 0), (3.5, 1), (7.5, 0), (8, 1), (12, 0), (12.5, 1), (16.5, 0), (18.5, 0)]\n",
    "            valence_seq = [(3, 1), (3.5, 1), (7.5, 1), (8, 1), (12, 1), (12.5, 0), (16.5, 0), (18.5, 1)]\n",
    "        anaerobic_bin[pid] = (arousal_seq, valence_seq)\n",
    "    return anaerobic_bin\n",
    "\n",
    "def combine_features(features_eda, features_ppg):\n",
    "    \"\"\"Combine EDA and PPG features.\"\"\"\n",
    "    # Merge on PID and categories\n",
    "    combined = pd.merge(\n",
    "        features_eda, \n",
    "        features_ppg, \n",
    "        on=['PID', 'arousal_category', 'valence_category', 'four_class_category'],\n",
    "        suffixes=('_eda', '_ppg')\n",
    "    )\n",
    "    return combined\n",
    "\n",
    "def create_four_class_dataset(features_df):\n",
    "    \"\"\"Create dataset focused on four-class classification.\"\"\"\n",
    "    # Filter to include only the four-class category\n",
    "    four_class_df = features_df.copy()\n",
    "    four_class_df = four_class_df[['PID', 'four_class_category'] + \n",
    "                                 [col for col in four_class_df.columns \n",
    "                                  if col not in ['PID', 'arousal_category', 'valence_category', 'four_class_category']]]\n",
    "    return four_class_df\n",
    "\n",
    "def save_datasets(*datasets):\n",
    "    \"\"\"Save all processed datasets.\"\"\"\n",
    "    dataset_names = [\n",
    "        \"Raw_EDA\", \"Raw_PPG\", \n",
    "        \"Features_EDA\", \"Features_PPG\", \"Features_Combined\",\n",
    "        \"Features_FourClass_EDA\", \"Features_FourClass_PPG\", \"Features_FourClass_Combined\"\n",
    "    ]\n",
    "    \n",
    "    for name, dataset in zip(dataset_names, datasets):\n",
    "        filename = OUTPUT_DIR / f\"{name}.csv\"\n",
    "        dataset.to_csv(filename, index=False)\n",
    "        print(f\"Saved: {filename}\")\n",
    "\n",
    "# Run the complete pipeline\n",
    "results = run_complete_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## 7. Summary and Next Steps\n",
    "\n",
    "The preprocessing pipeline has successfully:\n",
    "\n",
    "1. **Loaded and processed** raw physiological data from multiple conditions\n",
    "2. **Segmented** data according to experimental protocols\n",
    "3. **Extracted meaningful features** from EDA and PPG signals\n",
    "4. **Created categorical labels** for emotional states\n",
    "5. **Saved organized datasets** for further analysis\n",
    "\n",
    "### Output Files Created:\n",
    "```\n",
    "Processed_Datasets/\n",
    "├── Raw_EDA.csv\n",
    "├── Raw_PPG.csv \n",
    "├── Features_EDA.csv\n",
    "├── Features_PPG.csv \n",
    "├── Features_Combined.csv \n",
    "├── Features_FourClass_PPG.csv\n",
    "├── Features_FourClass_EDA.csv \n",
    "└── Features_FourClass_Combined.csv\n",
    "```\n",
    "\n",
    "### Four-Class Emotional States:\n",
    "- **Class 0**: Low arousal, low valence\n",
    "- **Class 1**: Low arousal, high valence  \n",
    "- **Class 2**: High arousal, low valence\n",
    "- **Class 3**: High arousal, high valence\n",
    "\n",
    "These datasets are now ready for machine learning model training and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## 8. Dataset Exploration\n",
    "\n",
    "Let's explore the created datasets to understand their structure and distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_datasets(results):\n",
    "    \"\"\"Explore the created datasets.\"\"\"\n",
    "    \n",
    "    print(\"=== Dataset Overview ===\\n\")\n",
    "    \n",
    "    for name, dataset in results.items():\n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  Shape: {dataset.shape}\")\n",
    "        if 'arousal_category' in dataset.columns:\n",
    "            print(f\"  Arousal distribution: {dataset['arousal_category'].value_counts().to_dict()}\")\n",
    "        if 'valence_category' in dataset.columns:\n",
    "            print(f\"  Valence distribution: {dataset['valence_category'].value_counts().to_dict()}\")\n",
    "        if 'four_class_category' in dataset.columns:\n",
    "            print(f\"  Four-class distribution: {dataset['four_class_category'].value_counts().to_dict()}\")\n",
    "        print()\n",
    "\n",
    "# Explore the datasets\n",
    "explore_datasets(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
