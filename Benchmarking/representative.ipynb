{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataframe creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "import neurokit2 as nk\n",
        "import scipy.stats as stats\n",
        "import cvxEDA.src.cvxEDA as cvxEDA\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df=pd.DataFrame()\n",
        "# df should atleast contain columns as PID, EEG, PPG, arousal_category, arousal_category, unique_id (this is the id for that particular sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Binning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# binning for arousal into 0 and 1 for the dataset. 0 conveying low arousal and 1 conveying high arousal\n",
        "# binning for valence into 0 and 1 for the dataset. 0 conveying negative valence and 1 conveying positive valence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_eda = df[['EDA','PID','unique_id','valence_category','arousal_category']]\n",
        "print(raw_eda.head())\n",
        "\n",
        "raw_ppg = df[['PPG','PID','unique_id','valence_category','arousal_category']]\n",
        "print(raw_ppg.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Pre-processing for raw signals\n",
        "\n",
        "1) winorization\n",
        "2) nk.clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats.mstats import winsorize\n",
        "\n",
        "eda_values = raw_eda['EDA'].tolist()\n",
        "print(f\"Max EDA before Win: {max(eda_values)}\")\n",
        "print(f\"Min EDA before Win: {min(eda_values)}\")\n",
        "\n",
        "raw_eda['EDA'] = winsorize(raw_eda['EDA'], limits=[0.01, 0.01]) \n",
        "#choosing the lower limit as we have relatively cleaned data\n",
        "\n",
        "eda_values = raw_eda['EDA'].tolist()\n",
        "print(f\"Max EDA after Win: {max(eda_values)}\")\n",
        "print(f\"Min EDA after Win: {min(eda_values)}\")\n",
        "\n",
        "sampling_freq_eda = None # sampling frequency for eda data\n",
        "eda_data = raw_eda['EDA'].to_numpy()\n",
        "eda_clean = nk.eda_clean(eda_data, sampling_rate=sampling_freq_eda ,method='neurokit')\n",
        "x_eda = np.array(eda_clean).reshape(-1, 1)\n",
        "raw_eda['EDA'] = x_eda.flatten()\n",
        "\n",
        "eda_values = raw_eda['EDA'].tolist()\n",
        "print(f\"Max EDA after nk.clean: {max(eda_values)}\")\n",
        "print(f\"Min EDA after nk.clean: {min(eda_values)}\")\n",
        "raw_eda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats.mstats import winsorize\n",
        "\n",
        "ppg_values = raw_ppg['PPG'].tolist()\n",
        "print(f\"Max PPG before Win: {max(ppg_values)}\")\n",
        "print(f\"Min PPG before Win: {min(ppg_values)}\")\n",
        "\n",
        "raw_ppg['PPG'] = winsorize(raw_ppg['PPG'], limits=[0.01, 0.01]) \n",
        "#choosing the lower limit as we have relatively cleaned data\n",
        "\n",
        "ppg_values = raw_ppg['PPG'].tolist()\n",
        "print(f\"Max PPG after Win: {max(ppg_values)}\")\n",
        "print(f\"Min PPG after Win: {min(ppg_values)}\")\n",
        "\n",
        "sampling_freq_ppg = # sampling frequency for ppg data\n",
        "ppg_data = raw_ppg['PPG'].to_numpy()\n",
        "ppg_clean = nk.ppg_clean(ppg_data, sampling_rate=sampling_freq_ppg)\n",
        "x_ppg = np.array(ppg_clean).reshape(-1, 1)\n",
        "raw_ppg['PPG'] = x_ppg.flatten()\n",
        "\n",
        "ppg_values = raw_ppg['PPG'].tolist()\n",
        "print(f\"Max PPG after nk.clean: {max(ppg_values)}\")\n",
        "print(f\"Min PPG after nk.clean: {min(ppg_values)}\")\n",
        "\n",
        "raw_ppg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "EDA and PPG signals have been cleaned, save them into CASE Dataset folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(raw_eda.isnull().any())\n",
        "print(raw_ppg.isnull().any())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_eda_final = (\n",
        "    raw_eda\n",
        "    .groupby('unique_id', sort=False)\n",
        "    .agg(\n",
        "        PID=('PID', 'first'),\n",
        "        arousal_category=('arousal_category', 'first'),\n",
        "        valence_category=('valence_category', 'first'),\n",
        "        Data=('EDA', list),\n",
        "    )\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# inspect\n",
        "raw_eda_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_eda_path = \"<path to save the raw eda file>\"\n",
        "raw_eda_final.to_csv(raw_eda_path,index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_ppg_final = (\n",
        "    raw_ppg\n",
        "    .groupby('unique_id', sort=False)\n",
        "    .agg(\n",
        "        PID=('PID', 'first'),\n",
        "        arousal_category=('arousal_category', 'first'),\n",
        "        valence_category=('valence_category', 'first'),\n",
        "        Data=('PPG', list),\n",
        "    )\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# inspect\n",
        "raw_ppg_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_ppg_path = \"<path to save the raw ppg file>\"\n",
        "raw_ppg_final.to_csv(raw_ppg_path,index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Window spliting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_eda_path = \"<path to load the raw eda file>\"\n",
        "raw_ppg_path = \"<path to load the raw ppg file>\"\n",
        "raw_eda_final = pd.read_csv(raw_eda_path)\n",
        "raw_ppg_final = pd.read_csv(raw_ppg_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EDA\n",
        "no_split = None # <number of splits to be done for each data sample>\n",
        "raw_eda_window = pd.DataFrame(columns=[\"PID\", \"arousal_category\", \"valence_category\", \"Data\"])\n",
        "\n",
        "for index, row in raw_eda_final.iterrows():\n",
        "    pid = row['PID']\n",
        "    ac = row['arousal_category']\n",
        "    vc = row['valence_category']\n",
        "    eda_bs = row['Data']\n",
        "    parts = eda_bs.split() \n",
        "    eda_bs = [float(part.strip(\"[], \")) for part in parts if part.strip(\"[], \")]\n",
        "    eda_l = len(eda_bs)\n",
        "    # print(len(eda_bs))\n",
        "    for s in range(0,no_split):\n",
        "        eda_temp = eda_bs[int((eda_l/no_split)*s):int((eda_l/no_split)*(s+1))]\n",
        "        # print(len(eda_temp))\n",
        "        new_row = {\"PID\": pid, \"arousal_category\": ac, \"valence_category\": vc, \"Data\": eda_temp}\n",
        "        new_row_df = pd.DataFrame([new_row])\n",
        "        raw_eda_window = pd.concat([raw_eda_window, new_row_df], ignore_index=True)\n",
        "    # break\n",
        "    \n",
        "raw_eda_window\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PPG\n",
        "no_split = None # <number of splits to be done for each data sample>\n",
        "raw_ppg_window = pd.DataFrame(columns=[\"PID\", \"arousal_category\", \"valence_category\", \"Data\"])\n",
        "\n",
        "for index, row in raw_ppg_final.iterrows():\n",
        "    pid = row['PID']\n",
        "    ac = row['arousal_category']\n",
        "    vc = row['valence_category']\n",
        "    ppg_bs = row['Data']\n",
        "    parts = ppg_bs.split() \n",
        "    ppg_bs = [float(part.strip(\"[], \")) for part in parts if part.strip(\"[], \")]\n",
        "    ppg_l = len(ppg_bs)\n",
        "    # print(len(eda_bs))\n",
        "    for s in range(0,no_split):\n",
        "        ppg_temp = ppg_bs[int((ppg_l/no_split)*s):int((ppg_l/no_split)*(s+1))]\n",
        "        # print(len(eda_temp))\n",
        "        new_row = {\"PID\": pid, \"arousal_category\": ac, \"valence_category\": vc, \"Data\": ppg_temp}\n",
        "        new_row_df = pd.DataFrame([new_row])\n",
        "        raw_ppg_window = pd.concat([raw_ppg_window, new_row_df], ignore_index=True)\n",
        "    # break\n",
        "    \n",
        "raw_ppg_window"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feature Extraction on these raw files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fet_eda = ['ku_eda','sk_eda','dynrange','slope','variance','entropy','insc','fd_mean','max_scr','min_scr','nSCR','meanAmpSCR','meanRespSCR','sumAmpSCR','sumRespSCR']\n",
        "fet_ppg = ['BPM', 'PPG_Rate_Mean', 'HRV_MedianNN','HRV_Prc20NN', 'HRV_MinNN', 'HRV_HTI', 'HRV_TINN', 'HRV_LF', 'HRV_VHF','HRV_LFn', 'HRV_HFn', 'HRV_LnHF', \n",
        "            'HRV_SD1SD2', 'HRV_CVI', 'HRV_PSS','HRV_PAS', 'HRV_PI', 'HRV_C1d', 'HRV_C1a', 'HRV_DFA_alpha1','HRV_MFDFA_alpha1_Width', 'HRV_MFDFA_alpha1_Peak',\n",
        "            'HRV_MFDFA_alpha1_Mean', 'HRV_MFDFA_alpha1_Max','HRV_MFDFA_alpha1_Delta', 'HRV_MFDFA_alpha1_Asymmetry', 'HRV_ApEn','HRV_ShanEn', 'HRV_FuzzyEn', 'HRV_MSEn', \n",
        "            'HRV_CMSEn', 'HRV_RCMSEn','HRV_CD', 'HRV_HFD', 'HRV_KFD', 'HRV_LZC']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# cvxEDA\n",
        "def eda_stats(y):\n",
        "    Fs = 15.625\n",
        "    yn = (y - y.mean()) / y.std()\n",
        "    [r, p, t, l, d, e, obj] = cvxEDA.cvxEDA(yn, 1. / Fs)\n",
        "    return [r, p, t, l, d, e, obj]\n",
        "\n",
        "def shannon_entropy(window):\n",
        "    p = np.abs(window) / np.sum(np.abs(window))\n",
        "    return -np.sum(p * np.log2(p + 1e-10))\n",
        "\n",
        "def first_derivative(signal):\n",
        "    if len(signal) > 1:\n",
        "        time_values = np.arange(len(signal))\n",
        "        first_derivative = np.gradient(signal, time_values)\n",
        "        return first_derivative\n",
        "    else:\n",
        "        return np.array([])\n",
        "\n",
        "\n",
        "def second_derivative(signal):\n",
        "    fd = first_derivative(signal)\n",
        "    time_values = np.arange(len(fd))\n",
        "    second_derivative = np.gradient(first_derivative)\n",
        "    return second_derivative\n",
        "\n",
        "\n",
        "def calculate_integral(window):\n",
        "    a = np.sum(np.abs(window))\n",
        "    return a\n",
        "\n",
        "def calculate_avg_power(window):\n",
        "    avg_power = np.mean(np.square(np.abs(window)))\n",
        "    return avg_power\n",
        "\n",
        "def calculate_arc_length(window):\n",
        "    diff_signal = np.diff(window)\n",
        "    arc_length = np.sum(np.sqrt(1 + np.square(diff_signal)))\n",
        "    return arc_length\n",
        "\n",
        "def slope(window):\n",
        "    if len(window) > 1:\n",
        "        time_values = np.arange(len(window))\n",
        "        slope, _ = np.polyfit(time_values, window, 1)\n",
        "        return slope\n",
        "    else:\n",
        "        return np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eda_fet = pd.DataFrame()\n",
        "\n",
        "for index, row in raw_eda_window.iterrows():\n",
        "    row_pi = {}\n",
        "    row_pi['PID'] = row['PID']\n",
        "    row_pi['arousal_category'] = row['arousal_category']\n",
        "    row_pi['valence_category'] = row['valence_category']\n",
        "    raw_eda = row['Data']\n",
        "    x = np.array(raw_eda)\n",
        "    eda_phasic = nk.eda_phasic(x, sampling_freq_eda)\n",
        "    scr = np.array(eda_phasic['EDA_Phasic'])\n",
        "    scl = np.array(eda_phasic['EDA_Tonic'])\n",
        "    x_axis = np.linspace(0, scl.shape[0]/sampling_freq_eda, scl.shape[0])\n",
        "    row_pi['mean'] = np.mean(x) # Mean\n",
        "    row_pi['std'] = np.std(x) # Standard Deviation\n",
        "    row_pi['min'] = np.min(x) # Minimum\n",
        "    row_pi['max'] = np.max(x) # Maximum\n",
        "    row_pi['median_eda'] = np.quantile(x,0.5) #median\n",
        "    row_pi['ku_eda'] = stats.kurtosis(x) #kurtosis\n",
        "    row_pi['sk_eda'] = stats.skew(x) #skewness\n",
        "    row_pi['dynrange'] = x.max()/x.min()#dynamic range\n",
        "    row_pi['slope'] = np.polyfit(x_axis,scl,1)[0] #slope\n",
        "    row_pi['variance'] = np.var(x) # Variance\n",
        "    row_pi['entropy'] = shannon_entropy(x) # Shannon Entropy\n",
        "    row_pi['insc'] = calculate_integral(x) # insc\n",
        "    fd = first_derivative(x)\n",
        "    row_pi['fd_mean'] = np.mean(fd)\n",
        "    row_pi['fd_std'] = np.std(fd)\n",
        "    \n",
        "    row_pi['max_scr'] = np.max(scr) #min\n",
        "    row_pi['min_scr'] = np.min(scr) #max\n",
        "    row_pi['mean_scr'] = np.mean(scr) # Mean\n",
        "    row_pi['sd_scr'] = np.std(scr) # Standard Deviation\n",
        "\n",
        "    _, info = nk.eda_peaks(scr, sampling_freq_eda) #scr peak\n",
        "    peaks = info['SCR_Peaks']\n",
        "    amplitude = info['SCR_Amplitude']\n",
        "    recovery = info['SCR_RecoveryTime']\n",
        "    \n",
        "    row_pi['nSCR'] = len(info['SCR_Peaks']) / (x.shape[0]/sampling_freq_eda/60) #to get the number of peaks per minute\n",
        "    row_pi['aucSCR'] = np.trapz(scr)\n",
        "    row_pi['meanAmpSCR'] = np.nanmean(amplitude)\n",
        "    row_pi['maxAmpSCR'] = np.nanmax(amplitude)\n",
        "    row_pi['meanRespSCR'] = np.nanmean(recovery)\n",
        "    row_pi['sumAmpSCR'] = np.nansum(amplitude) / (x.shape[0]/sampling_freq_eda/60) # per minute\n",
        "    row_pi['sumRespSCR'] = np.nansum(recovery) / (x.shape[0]/sampling_freq_eda/60) # per minute\n",
        "\n",
        "    #scl features\n",
        "    row_pi['max_scl'] = np.max(scl) #min\n",
        "    row_pi['min_scl'] = np.min(scl) #max\n",
        "    row_pi['mean_scl'] = np.mean(scl) # Mean\n",
        "    row_pi['sd_scl'] = np.std(scl) # Standard Deviation\n",
        "\n",
        "\n",
        "    new_row = pd.DataFrame(row_pi , index=[0])\n",
        "    eda_fet = pd.concat([eda_fet, new_row], ignore_index=True)\n",
        "    \n",
        "    \n",
        "eda_fet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eda_fet = eda_fet[[\"PID\",\"arousal_category\",\"valence_category\",'mean', 'std', 'min', 'max', 'median_eda', 'ku_eda', 'sk_eda',\n",
        "       'dynrange', 'slope', 'variance', 'entropy', 'insc', 'fd_mean', 'fd_std',\n",
        "       'max_scr', 'min_scr', 'mean_scr', 'sd_scr', 'nSCR', 'aucSCR',\n",
        "       'meanAmpSCR', 'maxAmpSCR', 'meanRespSCR', 'sumAmpSCR', 'sumRespSCR',\n",
        "       'max_scl', 'min_scl', 'mean_scl', 'sd_scl']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Define the columns you want to scale\n",
        "columns_to_scale = eda_fet.columns.difference([\"PID\",\"arousal_category\",\"valence_category\"])\n",
        "\n",
        "def scale_participant(df):\n",
        "    # Replace inf, -inf with NaN\n",
        "    df[columns_to_scale] = df[columns_to_scale].replace([np.inf, -np.inf], np.nan)\n",
        "    \n",
        "    # Replace NaN values with 0\n",
        "    df[columns_to_scale] = df[columns_to_scale].fillna(0)\n",
        "    \n",
        "    # Apply scaling\n",
        "    df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
        "    return df\n",
        "\n",
        "# Instantiate the scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Apply the scaling function to each participant's data\n",
        "eda_fet = eda_fet.groupby('PID').apply(scale_participant).reset_index(drop=True)\n",
        "\n",
        "eda_fet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fet_eda_path = \"<path to save the features extracted from eda>\"\n",
        "eda_fet.to_csv(fet_eda_path,index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"No of High arousal Sample: {eda_fet['arousal_category'].tolist().count(1)}\")\n",
        "print(f\"No of Low arousal Sample: {eda_fet['arousal_category'].tolist().count(0)}\")\n",
        "print(f\"No of Negative Valence Sample: {eda_fet['valence_category'].tolist().count(0)}\")\n",
        "print(f\"No of Positive Valence Sample: {eda_fet['valence_category'].tolist().count(1)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ppg_fet = pd.DataFrame()\n",
        "for index, row in raw_ppg_window.iterrows():\n",
        "    row_pi = {}\n",
        "    row_pi['PID'] = row['PID']\n",
        "    row_pi['arousal_category'] = row['arousal_category']\n",
        "    row_pi['valence_category'] = row['valence_category']\n",
        "    raw_ppg = row['Data']\n",
        "    ppg_clean = nk.ppg_clean(raw_ppg, sampling_rate=sampling_freq_ppg)\n",
        "    \n",
        "    if len(ppg_clean) > 10 * sampling_freq_ppg:  # Ensure at least 10 seconds of data\n",
        "        ppg_signals, ppg_info = nk.ppg_process(ppg_clean, sampling_rate=sampling_freq_ppg)\n",
        "        \n",
        "        # Perform the analysis\n",
        "        analyze_df = nk.ppg_analyze(ppg_signals, sampling_rate=sampling_freq_ppg)\n",
        "        \n",
        "        # Update your data with analysis results\n",
        "        row_pi.update(analyze_df.iloc[0])\n",
        "        heart_rate = ppg_signals['PPG_Rate']\n",
        "        bpm = heart_rate.mean()\n",
        "        # print(bpm)\n",
        "        row_pi['BPM'] = bpm\n",
        "        row_pi = pd.DataFrame(row_pi , index=[0])\n",
        "        ppg_fet = pd.concat([ppg_fet, row_pi], ignore_index=True)\n",
        "\n",
        "    else:\n",
        "        print(f\"Signal too short for analysis at index\")\n",
        "        continue\n",
        "    \n",
        "print(ppg_fet.keys())\n",
        "columns_with_nan = ppg_fet.columns[ppg_fet.isna().any()].tolist()\n",
        "print(columns_with_nan)\n",
        "\n",
        "ppg_fet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pulse_values = ppg_fet['PPG_Rate_Mean'].tolist()\n",
        "hrv_values = ppg_fet['HRV_RMSSD'].tolist()\n",
        "\n",
        "print(f\"Max Pulse Rate for a window: {max(pulse_values)}\")\n",
        "print(f\"Min Pulse Rate for a window: {min(pulse_values)}\")\n",
        "\n",
        "print(f\"Max HRV RMSSD for a window: {max(hrv_values)}\")\n",
        "print(f\"Min HRV RMSSD for a window: {min(hrv_values)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "relevant_features_ppg = [\"PID\",\"valence_category\",\"arousal_category\",'BPM', 'PPG_Rate_Mean', 'HRV_MedianNN',\n",
        "       'HRV_Prc20NN', 'HRV_MinNN', 'HRV_HTI', 'HRV_TINN', 'HRV_LF', 'HRV_VHF',\n",
        "       'HRV_LFn', 'HRV_HFn', 'HRV_LnHF', 'HRV_SD1SD2', 'HRV_CVI', 'HRV_PSS',\n",
        "       'HRV_PAS', 'HRV_PI', 'HRV_C1d', 'HRV_C1a', 'HRV_DFA_alpha1',\n",
        "       'HRV_MFDFA_alpha1_Width', 'HRV_MFDFA_alpha1_Peak',\n",
        "       'HRV_MFDFA_alpha1_Mean', 'HRV_MFDFA_alpha1_Max',\n",
        "       'HRV_MFDFA_alpha1_Delta', 'HRV_MFDFA_alpha1_Asymmetry', 'HRV_ApEn',\n",
        "       'HRV_ShanEn', 'HRV_FuzzyEn', 'HRV_MSEn', 'HRV_CMSEn', 'HRV_RCMSEn',\n",
        "       'HRV_CD', 'HRV_HFD', 'HRV_KFD', 'HRV_LZC']\n",
        "\n",
        "ppg_fet = ppg_fet[relevant_features_ppg]\n",
        "\n",
        "# Define the columns you want to scale\n",
        "columns_to_scale = ppg_fet.columns.difference([\"PID\",\"arousal_category\",\"valence_category\"])\n",
        "\n",
        "def scale_participant(df):\n",
        "    # Replace inf, -inf with NaN\n",
        "    df[columns_to_scale] = df[columns_to_scale].replace([np.inf, -np.inf], np.nan)\n",
        "    # Replace NaN values with 0\n",
        "    df[columns_to_scale] = df[columns_to_scale].fillna(0)\n",
        "    # Apply scaling\n",
        "    df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
        "    return df\n",
        "\n",
        "# Instantiate the scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Apply the scaling function to each participant's data\n",
        "ppg_fet = ppg_fet.groupby('PID').apply(scale_participant).reset_index(drop=True)\n",
        "\n",
        "ppg_fet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fet_ppg_path = \"<path to save the features extracted from ppg>\"\n",
        "ppg_fet.to_csv(fet_ppg_path,index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_eda_path = \"<path to save the raw data from eda>\"\n",
        "raw_ppg_path = \"<path to save the raw data from ppg>\"\n",
        "raw_ppg_window.to_csv(raw_ppg_path,index=False)\n",
        "raw_eda_window.to_csv(raw_eda_path,index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "combined_fet = pd.concat([ppg_fet, eda_fet], axis=1)\n",
        "combined_fet = combined_fet.loc[:, ~combined_fet.columns.duplicated()]\n",
        "combined_fet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fet_com_path = \"<path to save the fetaures extracted from EDA and PPG as combined>\"\n",
        "combined_fet.to_csv(fet_com_path,index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Features and Raw Signals Data saved. We have created as possible window split for each category of data in order to increase data points while training. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1) TSNE\n",
        "2) UMAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Get the absolute path to the \"Scripts\" folder\n",
        "current_dir = os.getcwd()\n",
        "scripts_dir = os.path.join(current_dir, \"..\", \"Scripts\")\n",
        "sys.path.append(scripts_dir)\n",
        "\n",
        "from visualize import visualize\n",
        "\n",
        "# from import visualize\n",
        "\n",
        "fet_eda_path = \"<path to load the features for eda data>\"\n",
        "fet_ppg_path = \"<path to load the features for ppg data>\"\n",
        "fet_com_path = \"<path to load the features for eda+ppg data>\"\n",
        "\n",
        "output_folder = \"<path to output folder for results>\"\n",
        "\n",
        "# EDA\n",
        "visualize(input_path = fet_eda_path, output_folder = output_folder + \"EDA_\")\n",
        "\n",
        "# PPG\n",
        "visualize(input_path = fet_ppg_path, output_folder = output_folder + \"PPG_\")\n",
        "\n",
        "# Combined\n",
        "visualize(input_path = fet_com_path, output_folder = output_folder + \"Combined_\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3) In-distribution (ID) datapoints and uncertinity in the features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Confidence Intervals (CIs)\n",
        "\n",
        "import sys\n",
        "import os\n",
        "# Get the absolute path to the \"Scripts\" folder\n",
        "current_dir = os.getcwd()\n",
        "scripts_dir = os.path.join(current_dir, \"..\", \"Scripts\")\n",
        "sys.path.append(scripts_dir)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from data_suite import Data_SUITE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def train_and_predict(X_train, X_test, features):\n",
        "    ds = Data_SUITE(\n",
        "        copula_type='vine',\n",
        "        n_copula_samples=2000,\n",
        "        representer='pca',\n",
        "        rep_dim=10\n",
        "    )\n",
        "    \n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    ds.fit(X_train_scaled)\n",
        "    \n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    conformal_dict, _ = ds.predict(X_test_scaled)\n",
        "    \n",
        "    inconsistent = {}\n",
        "    uncertainty = {}\n",
        "    feature_ranges = X_train.max(axis=0) - X_train.min(axis=0)\n",
        "    \n",
        "    for feat_idx in conformal_dict:\n",
        "        feat_name = features.columns[feat_idx]\n",
        "        lower = conformal_dict[feat_idx]['min']\n",
        "        upper = conformal_dict[feat_idx]['max']\n",
        "        test_vals = X_test_scaled[:, feat_idx]\n",
        "        inconsistent[feat_name] = np.mean((test_vals < lower) | (test_vals > upper))\n",
        "        ci_width = upper - lower\n",
        "        uncertainty[feat_name] = np.median(ci_width / feature_ranges[feat_idx])\n",
        "    \n",
        "    return inconsistent, uncertainty\n",
        "\n",
        "\n",
        "def CI(data, strat):\n",
        "    identifiers = data[['PID','arousal_category','valence_category']]\n",
        "    if strat == 'arousal_category':\n",
        "        features = data.drop(columns=['PID','valence_category'])\n",
        "    else:\n",
        "        features = data.drop(columns=['PID','arousal_category'])\n",
        "    \n",
        "    # Train-test split (stratify by original distribution)\n",
        "    X_train_1, X_test_1 = train_test_split(features, test_size=0.5, random_state=42, stratify=features[[strat]])\n",
        "    X_train_2, X_test_2 = X_test_1, X_train_1 \n",
        "    \n",
        "    inconsistent_1, uncertainty_1 = train_and_predict(X_train_1, X_test_1, features)\n",
        "    inconsistent_2, uncertainty_2 = train_and_predict(X_train_2, X_test_2, features)\n",
        "\n",
        "    # Compute average values\n",
        "    avg_inconsistent = {k: (inconsistent_1.get(k, 0) + inconsistent_2.get(k, 0)) / 2 for k in set(inconsistent_1) | set(inconsistent_2)}\n",
        "    avg_uncertainty = {k: (uncertainty_1.get(k, 0) + uncertainty_2.get(k, 0)) / 2 for k in set(uncertainty_1) | set(uncertainty_2)}\n",
        "    \n",
        "    return avg_inconsistent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fet_eda_path = \"<path to load the features for eda data>\"\n",
        "eda_fet = pd.read_csv(fet_eda_path)\n",
        "ci_eda_ar = CI(data = eda_fet, strat = \"arousal_category\")\n",
        "ci_eda_va = CI(data = eda_fet, strat = \"valence_category\")\n",
        "\n",
        "fet_ppg_path = \"<path to load the features for ppg data>\"\n",
        "ppg_fet = pd.read_csv(fet_ppg_path)\n",
        "ci_ppg_ar = CI(data = ppg_fet, strat = \"arousal_category\")\n",
        "ci_ppg_va = CI(data = ppg_fet, strat = \"valence_category\")\n",
        "\n",
        "fet_com_path =  \"<path to load the features for eda+ppg data>\"\n",
        "com_fet = pd.read_csv(fet_com_path)\n",
        "ci_com_ar = CI(data = com_fet, strat = \"arousal_category\")\n",
        "ci_com_va = CI(data = com_fet, strat = \"valence_category\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pprint\n",
        "\n",
        "di_path = \"<path to save the Feature Confidence Intervals output>\"\n",
        "with open(di_path, \"w\") as f:\n",
        "    # Write each dictionary with a blank line in between\n",
        "    f.write(\"The CIs for Combined's Features keeping stratify as Valence\\n\")\n",
        "    pprint.pprint(ci_com_va, stream=f)\n",
        "    f.write(\"\\n\")\n",
        "    \n",
        "    f.write(\"The CIs for Combined's Features keeping stratify as Arousal\\n\")\n",
        "    pprint.pprint(ci_com_ar, stream=f)\n",
        "    f.write(\"\\n\")\n",
        "    \n",
        "    f.write(\"The CIs for PPG's Features keeping stratify as Valence\\n\")\n",
        "    pprint.pprint(ci_ppg_va, stream=f)\n",
        "    f.write(\"\\n\")\n",
        "    \n",
        "    f.write(\"The CIs for PPG's Features keeping stratify as Arousal\\n\")\n",
        "    pprint.pprint(ci_ppg_ar, stream=f)\n",
        "    f.write(\"\\n\")\n",
        "    \n",
        "    f.write(\"The CIs for EDA's Features keeping stratify as Valence\\n\")\n",
        "    pprint.pprint(ci_eda_va, stream=f)\n",
        "    f.write(\"\\n\")\n",
        "    \n",
        "    f.write(\"The CIs for EDA's Features keeping stratify as Arousal\\n\")\n",
        "    pprint.pprint(ci_eda_ar, stream=f)\n",
        "    f.write(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"The features which are highly variable(inconsistent) wrt to Valence in Combined\")\n",
        "print({k: f\"{v:.1%}\" for k,v in ci_com_va.items() if v > 0.10})\n",
        "\n",
        "print(\"The features which are highly variable(inconsistent) wrt to Arousal in Combined\")\n",
        "print({k: f\"{v:.1%}\" for k,v in ci_com_ar.items() if v > 0.10})\n",
        "\n",
        "print(\"The features which are highly variable(inconsistent) wrt to Valence in PPG\")\n",
        "print({k: f\"{v:.1%}\" for k,v in ci_ppg_va.items() if v > 0.10})\n",
        "\n",
        "print(\"The features which are highly variable(inconsistent) wrt to Arousal in PPG\")\n",
        "print({k: f\"{v:.1%}\" for k,v in ci_ppg_ar.items() if v > 0.10})\n",
        "\n",
        "print(\"The features which are highly variable(inconsistent) wrt to Valence in EDA\")\n",
        "print({k: f\"{v:.1%}\" for k,v in ci_eda_va.items() if v > 0.10})\n",
        "\n",
        "print(\"The features which are highly variable(inconsistent) wrt to Arousal in EDA\")\n",
        "print({k: f\"{v:.1%}\" for k,v in ci_eda_ar.items() if v > 0.10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Data distribution - per window - HVR SD1SD2, EDA MEAN - Histogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "fet_eda_path = \"<path to load the features for eda data>\"\n",
        "eda_fet = pd.read_csv(fet_eda_path)\n",
        "fet_ppg_path = \"<path to load the features for ppg data>\"\n",
        "ppg_fet = pd.read_csv(fet_ppg_path)\n",
        "fet_com_path = \"<path to load the features for eda+ppg data>\"\n",
        "com_fet = pd.read_csv(fet_com_path)\n",
        "# ----- Plot for EDA Means -----\n",
        "eda_mean_path = \"<path to save the results>\"\n",
        "eda_means = eda_fet['mean'].tolist()  # List of float values\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(eda_means, bins=20, edgecolor='black', alpha=0.75)\n",
        "plt.title('Distribution of EDA Means')\n",
        "plt.xlabel('EDA Mean')\n",
        "plt.ylabel('Windows')\n",
        "\n",
        "# Create x-ticks using np.arange. Rounding min and max to integers.\n",
        "ticks = np.arange(int(np.floor(min(eda_means))), int(np.ceil(max(eda_means))), 1)\n",
        "plt.xticks(ticks)\n",
        "\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.savefig(eda_mean_path)\n",
        "plt.close()\n",
        "\n",
        "# ----- Plot for HRV_SD1SD2 -----\n",
        "hrv_sd1sd2_path = \"<path to save the resultsF\"\n",
        "hrv_sd1sd2 = ppg_fet['HRV_SD1SD2'].tolist()  # List of float values\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(hrv_sd1sd2, bins=20, edgecolor='black', alpha=0.75)\n",
        "plt.title('Distribution of HRV_SD1SD2')\n",
        "plt.xlabel('HRV_SD1SD2')\n",
        "plt.ylabel('Windows')\n",
        "\n",
        "# Create x-ticks for HRV_SD1SD2\n",
        "ticks = np.arange(int(np.floor(min(hrv_sd1sd2))), int(np.ceil(max(hrv_sd1sd2))), 1)\n",
        "plt.xticks(ticks)\n",
        "\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.savefig(hrv_sd1sd2_path)\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Artifacts Detection on the Raw Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "EDA Artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "# Get the absolute path to the \"Scripts\" folder\n",
        "current_dir = os.getcwd()\n",
        "scripts_dir = os.path.join(current_dir, \"..\", \"Scripts\")\n",
        "sys.path.append(scripts_dir)\n",
        "\n",
        "import eda_artifact\n",
        "\n",
        "out_path = \"<path to save the eda artifacts results>\"\n",
        "data_file_eda = \"<path to load the raw eda data>\"\n",
        "window_size = None # the window size chosen\n",
        "SAMPLE_RATE = None # sampling rate chosen\n",
        "average_artifact, sd_artifact= eda_artifact.main(data_file = data_file_eda,window_size = window_size, out_path = out_path, SAMPLE_RATE = SAMPLE_RATE )\n",
        "print(average_artifact, sd_artifact)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eda_art_df = pd.read_csv(out_path)\n",
        "pid_list = list(set(eda_art_df['PID'].tolist()))\n",
        "art_list_p = []\n",
        "for i in pid_list: \n",
        "    art_list = eda_art_df[eda_art_df['PID'] == i]['Artifact (%)'].tolist()\n",
        "    art_pid = [0 if j == \"Not Applicable (not enough data)\" else float(j) for j in art_list]\n",
        "    art_list_p.extend(art_pid)\n",
        "    art_pid = sum(art_pid)\n",
        "    count_pid = eda_art_df['PID'].tolist().count(i)\n",
        "    print(f\"The total percent of artifacts in Participant ID: {i} is {art_pid/count_pid}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "average_artifact_p = sum(art_list_p)/len(art_list_p)\n",
        "sd_artifact_p = np.std(art_list_p)\n",
        "\n",
        "print(f\"The average percent (across window) of EDA artifacts {average_artifact}%\")\n",
        "print(f\"The standard deviation (across window) of EDA artifacts {sd_artifact}%\")\n",
        "\n",
        "print(f\"The average percent (across participants) of EDA artifacts {average_artifact_p}%\")\n",
        "print(f\"The standard deviation (across participants) of EDA artifacts {sd_artifact_p}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PPG Artifacts (only Motion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "# Get the absolute path to the \"Scripts\" folder\n",
        "current_dir = os.getcwd()\n",
        "scripts_dir = os.path.join(current_dir, \"..\", \"Scripts\")\n",
        "sys.path.append(scripts_dir)\n",
        "\n",
        "import ppg_artifact\n",
        "\n",
        "out_path = \"<path to save the ppg artifacts results>\"\n",
        "data_file_ppg = \"<path to load the raw ppg file>\"\n",
        "window_size = None # the window size chosen\n",
        "ppg_artifact.find_ppg_artifact(input_path = data_file_ppg, output_path = out_path, window_size = window_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ppg_art_df = pd.read_csv(out_path)\n",
        "pid_list = list(set(ppg_art_df['PID'].tolist()))\n",
        "art_list_p = []\n",
        "for i in pid_list: \n",
        "    art_list = ppg_art_df[ppg_art_df['PID'] == i]['Artifact (%)'].tolist()\n",
        "    art_pid = [0 if j == \"Not Applicable (not enough data)\" else float(j) for j in art_list]\n",
        "    art_list_p.extend(art_pid)\n",
        "    art_pid = sum(art_pid)\n",
        "    count_pid = ppg_art_df['PID'].tolist().count(i)\n",
        "    print(f\"The total percent of artifacts in Participant ID: {i} is {art_pid/count_pid}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "average_artifact_p = sum(art_list_p)/len(art_list_p)\n",
        "sd_artifact_p = np.std(art_list_p)\n",
        "\n",
        "print(f\"The average percent (across participants) of PPG artifacts (motion) {average_artifact_p}%\")\n",
        "print(f\"The standard deviation (across participants) of PPG artifacts (motion) {sd_artifact_p}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Normalizing (Z-Score) the Raw Signal to be used for Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def downsample_list(lst, original_rate=1000, target_rate=256):\n",
        "    factor = original_rate / target_rate\n",
        "    return [lst[int(i * factor)] for i in range(int(len(lst) / factor))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def downsample_dataframe(df, original_rate=1000, target_rate=256):\n",
        "    df['Data'] = df['Data'].apply(lambda lst: downsample_list(lst, original_rate, target_rate))\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_eda_path = \"<path to the raw eda data file>\"\n",
        "raw_ppg_path = \"<path to the raw ppg data file>\"\n",
        "raw_eda = pd.read_csv(raw_eda_path)\n",
        "raw_ppg = pd.read_csv(raw_ppg_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_str_to_list(temp_df):\n",
        "    for index, row in temp_df.iterrows():\n",
        "        eda_bs = row['Data']\n",
        "        parts = eda_bs.split() \n",
        "        eda_bs = [float(part.strip(\"[], \")) for part in parts if part.strip(\"[], \")]\n",
        "        temp_df.at[index, 'Data'] = eda_bs\n",
        "        \n",
        "    return temp_df\n",
        "\n",
        "def normalize_list(lst, mean, std):\n",
        "    return [(x - mean) / std for x in lst]\n",
        "    \n",
        "def z_score(temp_df):\n",
        "    all_values = [val for sublist in temp_df['Data'] for val in sublist]\n",
        "    mean = np.mean(all_values)\n",
        "    std = np.std(all_values)\n",
        "    temp_df['Data'] = temp_df['Data'].apply(lambda lst: normalize_list(lst, mean, std))\n",
        "    return temp_df  \n",
        "\n",
        "normalized_eda = pd.DataFrame()\n",
        "normalized_ppg = pd.DataFrame()  \n",
        "\n",
        "pid_list = list(set(raw_eda['PID'].tolist()))\n",
        "for pid in pid_list:\n",
        "    raw_eda_pid = raw_eda[raw_eda['PID'] == pid]\n",
        "    raw_eda_pid = convert_str_to_list(temp_df=raw_eda_pid)\n",
        "    raw_eda_pid = z_score(temp_df=raw_eda_pid)\n",
        "\n",
        "    raw_ppg_pid = raw_ppg[raw_ppg['PID'] == pid]\n",
        "    raw_ppg_pid = convert_str_to_list(temp_df=raw_ppg_pid)\n",
        "    raw_ppg_pid = z_score(temp_df=raw_ppg_pid)\n",
        "\n",
        "    normalized_eda = pd.concat([normalized_eda, raw_eda_pid], ignore_index=True)\n",
        "    normalized_ppg = pd.concat([normalized_ppg, raw_ppg_pid], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "normalized_eda.to_csv(raw_eda_path,index=False)\n",
        "normalized_ppg.to_csv(raw_ppg_path,index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_eda_path = \"<path to the raw eda data file>\"\n",
        "raw_ppg_path = \"<path to the raw ppg data file>\"\n",
        "raw_eda = pd.read_csv(raw_eda_path)\n",
        "raw_ppg = pd.read_csv(raw_ppg_path)\n",
        "raw_ppg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For Benchmarking on ML and DL models for Raw and Features of the a dataset, run the following command in the terminal. \n",
        "\n",
        "python3 Model_running.py --eda_fet_path <eda_fet_path> --ppg_fet_path <ppg_fet_path> --com_fet_path <com_det_path> --eda_raw_path <eda_raw_path> --ppg_raw_path <ppg_raw_path> --out_path_benchmark <out_path_benchmark> \\\n",
        "\n",
        "Then the following cell with the <out_path_benchmark> as read_csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"<Path to benchamarking outputs result>\")\n",
        "\n",
        "# Group by and compute mean and std\n",
        "grouped = df.groupby(['Classification','Signal Type','Input Type', 'Model Name'])[['Test_Accuracy', 'Test_F1']].agg(['mean', 'std'])\n",
        "\n",
        "# Format mean ± std for both Accuracy and F1 in same row, multiplied by 100\n",
        "formatted = grouped.apply(\n",
        "    lambda x: pd.Series({\n",
        "        'Accuracy ± Std | F1 ± Std': \n",
        "        f\"{x[('Test_Accuracy', 'mean')]*100:.3f} ± {x[('Test_Accuracy', 'std')]*100:.3f} | {x[('Test_F1', 'mean')]:.3f} ± {x[('Test_F1', 'std')]:.3f}\"\n",
        "    }), axis=1\n",
        ")\n",
        "\n",
        "print(formatted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "BenchMarking CLSP Models (zero-shot) on the Features of the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "# Get the absolute path to the \"Scripts\" folder\n",
        "current_dir = os.getcwd()\n",
        "scripts_dir = os.path.join(current_dir, \"..\", \"Scripts\")\n",
        "sys.path.append(scripts_dir)\n",
        "import clsp_com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CLSP Zero-Shot on Combined Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Zero-shot on CLSP for Combined Features\")\n",
        "\n",
        "com_fet_path = \"<Path to eda+ppg extracted features file>\"\n",
        "clsp_com.Arousal(com_path= com_fet_path)\n",
        "clsp_com.Valence(com_path=com_fet_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CLSP Zero-Shot on PPG Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import clsp_ppg\n",
        "\n",
        "print(\"Zero-shot on CLSP for PPG Features\")\n",
        "\n",
        "ppg_fet_path = \"<Path to ppg extracted features file>\"\n",
        "clsp_ppg.Arousal(ppg_path= ppg_fet_path)\n",
        "clsp_ppg.Valence(ppg_path=ppg_fet_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CLSP Zero-shot for CLSP on EDA Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import clsp_eda\n",
        "\n",
        "print(\"Zero-shot on CLSP for EDA Features\")\n",
        "\n",
        "eda_fet_path = \"<Path to eda extracted features file>\"\n",
        "clsp_eda.Arousal(eda_path= eda_fet_path)\n",
        "clsp_eda.Valence(eda_path=eda_fet_path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
