{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584de6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c148ddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_models(csv_path):\n",
    "    # Load CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Drop unwanted columns\n",
    "    df = df.drop(columns=[\"Unnamed: 0\", \"PID\"], errors=\"ignore\")\n",
    "\n",
    "    # Extract identifier\n",
    "    targets = ['arousal_category', 'valence_category']\n",
    "    df = df.dropna(subset=targets + ['<dataset name colum>'])  # Remove rows with missing labels\n",
    "    feature_cols = df.drop(columns=targets + ['<dataset name colum>']).columns\n",
    "\n",
    "    # Initialize models\n",
    "    models = {\n",
    "        'MLP': MLPClassifier(random_state=42, max_iter=100),\n",
    "        'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    }\n",
    "\n",
    "    # Store metrics\n",
    "    results = {target: defaultdict(list) for target in targets}\n",
    "\n",
    "    # Leave-one-dataset-out cross-validation\n",
    "    unique_ids = df['<dataset name colum>'].unique()\n",
    "\n",
    "    for uid in unique_ids:\n",
    "        test_idx = df['<dataset name colum>'] == uid\n",
    "        train_idx = ~test_idx\n",
    "\n",
    "        X_train = df.loc[train_idx, feature_cols]\n",
    "        X_test = df.loc[test_idx, feature_cols]\n",
    "\n",
    "        if X_train.empty or X_test.empty:\n",
    "            print(f\"Skipping <dataset name colum> {uid} due to empty train/test split.\")\n",
    "            continue\n",
    "\n",
    "        X_train_scaled = X_train  # You may want to scale later\n",
    "        X_test_scaled = X_test\n",
    "\n",
    "        for target in targets:\n",
    "            y_train = df.loc[train_idx, target]\n",
    "            y_test = df.loc[test_idx, target]\n",
    "\n",
    "            # Skip if training or test labels are missing or insufficient\n",
    "            if y_train.nunique() < 2:\n",
    "                print(f\"Skipping target {target} for <dataset name colum> {uid} due to insufficient class diversity.\")\n",
    "                continue\n",
    "\n",
    "            for model_name, model in models.items():\n",
    "                try:\n",
    "                    model.fit(X_train_scaled, y_train)\n",
    "                    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "                    acc = accuracy_score(y_test, y_pred)\n",
    "                    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "                    results[target][model_name + '_accuracy'].append(acc)\n",
    "                    results[target][model_name + '_f1'].append(f1)\n",
    "                except ValueError as e:\n",
    "                    print(f\"Model {model_name} failed on <dataset name colum> {uid} for target {target}: {e}\")\n",
    "                    continue\n",
    "\n",
    "    # Report average metrics\n",
    "    for target in targets:\n",
    "        print(f\"\\nResults for target: {target}\")\n",
    "        for metric in results[target]:\n",
    "            scores = results[target][metric]\n",
    "            if scores:\n",
    "                avg_score = np.mean(scores)\n",
    "                print(f\"{metric}: {avg_score:.4f}\")\n",
    "            else:\n",
    "                print(f\"{metric}: No valid results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fefc411",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Results for transferability within a group\\n\")\n",
    "csv_path = \"<path to csv for the group of datasets chosen for eda+ppg>\"\n",
    "print(\"Results for Combined\")\n",
    "evaluate_models(csv_path)\n",
    "print()\n",
    "csv_path = \"<path to csv for the group of datasets chosen for eda>\"\n",
    "print(\"Results for EDA\")\n",
    "evaluate_models(csv_path)\n",
    "print()\n",
    "csv_path = \"<path to csv for the group of datasets chosen for ppg>\"\n",
    "print(\"Results for PPG\")\n",
    "evaluate_models(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e843db9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Get the absolute path to the \"Scripts\" folder\n",
    "current_dir = os.getcwd()\n",
    "scripts_dir = os.path.join(current_dir, \"..\", \"Scripts\")\n",
    "sys.path.append(scripts_dir)\n",
    "import clsp_com\n",
    "import clsp_eda\n",
    "import clsp_ppg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7c1ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Zero-shot on CLSP on <group name>\")\n",
    "\n",
    "print(\"<Group Name>\")\n",
    "eda_fet_path = \"<path to csv for eda features for the group>\"\n",
    "clsp_eda.Arousal(eda_path= eda_fet_path)\n",
    "clsp_eda.Valence(eda_path=eda_fet_path)\n",
    "ppg_fet_path = \"<path to csv for ppg features>\"\n",
    "clsp_ppg.Arousal(ppg_path= ppg_fet_path)\n",
    "clsp_ppg.Valence(ppg_path=ppg_fet_path)\n",
    "com_fet_path = \"<path to csv for eda+ppg features for the group>\"\n",
    "clsp_com.Arousal(com_path= com_fet_path)\n",
    "clsp_com.Valence(com_path=com_fet_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
